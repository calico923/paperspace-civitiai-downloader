"""
Model Metadata Scanner

ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Civitaiã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’æŠ½å‡ºãƒ»åˆ†é¡ã™ã‚‹æ©Ÿèƒ½
ComfyUI-Lora-Managerã®å®Ÿè£…ã‚’å‚è€ƒã«ç°¡ç•¥åŒ–
"""

import os
import json
import hashlib
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import aiohttp
import asyncio
import time
import random

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ModelMetadata:
    """ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æ§‹é€ """
    file_name: str
    file_path: str
    file_size: int
    sha256: str
    model_type: str  # lora, checkpoint, embedding
    base_model: str
    civitai_url: Optional[str] = None
    download_urls: List[str] = None
    model_id: Optional[int] = None
    version_id: Optional[int] = None
    model_name: Optional[str] = None
    description: Optional[str] = None
    tags: List[str] = None
    creator: Optional[str] = None
    nsfw_level: int = 0
    from_civitai: bool = False
    
    def __post_init__(self):
        if self.download_urls is None:
            self.download_urls = []
        if self.tags is None:
            self.tags = []

class ModelMetadataScanner:
    """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: Civitai APIã‚­ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        self.api_key = api_key
        self.base_url = "https://civitai.com/api/v1"
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        self.supported_extensions = {
            '.safetensors', '.ckpt', '.pt', '.pth', '.bin'
        }
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.model_type_keywords = {
            'lora': ['lora', 'locon', 'loha'],
            'checkpoint': ['checkpoint', 'model'],
            'embedding': ['embedding', 'textualinversion', 'ti']
        }
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼"""
        await self._create_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†"""
        await self.close()
    
    async def _create_session(self):
        """HTTP sessionã‚’ä½œæˆ"""
        connector = aiohttp.TCPConnector(
            ssl=True,
            limit=8,
            ttl_dns_cache=300,
            force_close=False,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(
            total=None,
            connect=60,
            sock_read=300
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
    
    async def close(self):
        """HTTP sessionã‚’é–‰ã˜ã‚‹"""
        if self.session:
            await self.session.close()
    
    def _get_headers(self, use_auth: bool = True) -> Dict[str, str]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        headers = {
            'User-Agent': 'Model-Metadata-Scanner/1.0'
        }
        
        if use_auth and self.api_key:
            headers['Authorization'] = f'Bearer {self.api_key}'
            headers['Content-Type'] = 'application/json'
        
        return headers
    
    def _calculate_sha256(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        sha256_hash = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except Exception as e:
            logger.error(f"SHA256è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""
    
    def _detect_model_type(self, file_path: str, file_name: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        file_name_lower = file_name.lower()
        file_path_lower = file_path.lower()
        
        # ã¾ãšãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‹ã‚‰åˆ¤å®š
        if '/loras/' in file_path_lower or '\\loras\\' in file_path_lower:
            return "lora"
        elif '/checkpoints/' in file_path_lower or '\\checkpoints\\' in file_path_lower:
            return "checkpoint"
        elif '/embeddings/' in file_path_lower or '\\embeddings\\' in file_path_lower:
            return "embedding"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã§åˆ¤å®šã§ããªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰åˆ¤å®š
        for model_type, keywords in self.model_type_keywords.items():
            for keyword in keywords:
                if keyword in file_name_lower:
                    return model_type
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯checkpoint
        return "checkpoint"
    
    def _detect_base_model(self, file_path: str, file_name: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’åˆ¤å®š"""
        file_name_lower = file_name.lower()
        
        if 'sdxl' in file_name_lower:
            return 'SDXL'
        elif 'sd3' in file_name_lower:
            return 'SD3'
        elif 'sd2' in file_name_lower:
            return 'SD2.1'
        elif 'sd1' in file_name_lower or 'sd15' in file_name_lower:
            return 'SD1.5'
        else:
            return 'Unknown'
    
    async def _search_model_by_hash(self, sha256: str) -> Optional[Dict]:
        """SHA256ãƒãƒƒã‚·ãƒ¥ã§Civitaiã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ï¼ˆè¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰"""
        if not self.session or not sha256:
            return None
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒªã‚¹ãƒˆï¼ˆCivitai API â†’ CivArchiveï¼‰
        providers = [
            ("Civitai API", self._search_civitai_api),
            ("CivArchive", self._search_civarchive_api)
        ]
        
        last_error = None
        
        for provider_name, search_func in providers:
            try:
                logger.info(f"{provider_name}ã§æ¤œç´¢ä¸­: {sha256[:16]}...")
                result = await search_func(sha256)
                
                if result:
                    logger.info(f"{provider_name}ã§æ¤œç´¢æˆåŠŸ!")
                    return result
                else:
                    logger.warning(f"{provider_name}ã§æ¤œç´¢å¤±æ•—")
                    
            except Exception as e:
                logger.warning(f"{provider_name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                last_error = str(e)
                continue
        
        logger.warning(f"ã™ã¹ã¦ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§æ¤œç´¢å¤±æ•—: {last_error}")
        return None
    
    async def _search_civitai_api(self, sha256: str) -> Optional[Dict]:
        """Civitai APIã§æ¤œç´¢ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰"""
        max_retries = 3
        base_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                url = f"{self.base_url}/model-versions/by-hash/{sha256}"
                async with self.session.get(url, headers=self._get_headers()) as response:
                    if response.status == 200:
                        version_info = await response.json()
                        
                        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚‚å–å¾—
                        model_id = version_info.get('modelId')
                        if model_id:
                            model_url = f"{self.base_url}/models/{model_id}"
                            async with self.session.get(model_url, headers=self._get_headers()) as model_response:
                                if model_response.status == 200:
                                    model_data = await model_response.json()
                                    if 'model' not in version_info:
                                        version_info['model'] = {}
                                    version_info['model']['description'] = model_data.get('description')
                                    version_info['model']['tags'] = model_data.get('tags', [])
                                    version_info['creator'] = model_data.get('creator')
                        
                        return version_info
                    elif response.status == 404:
                        logger.debug(f"Civitai API: ãƒãƒƒã‚·ãƒ¥ {sha256[:16]}... ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        return None
                    elif response.status == 401:
                        logger.warning(f"Civitai API: èªè¨¼ã‚¨ãƒ©ãƒ¼")
                        return None
                    elif response.status == 429:
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤
                        retry_after = int(response.headers.get('Retry-After', 60))
                        delay = min(retry_after, 300)  # æœ€å¤§5åˆ†
                        logger.warning(f"Civitai API: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ (è©¦è¡Œ {attempt + 1}/{max_retries}), {delay}ç§’å¾…æ©Ÿ")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(delay)
                            continue
                        else:
                            return None
                    elif response.status == 500:
                        # ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯çŸ­ã„å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤
                        delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                        logger.warning(f"Civitai API: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries}), {delay:.1f}ç§’å¾…æ©Ÿ")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(delay)
                            continue
                        else:
                            return None
                    else:
                        logger.warning(f"Civitai API: ã‚¨ãƒ©ãƒ¼ {response.status}")
                        return None
            except Exception as e:
                logger.error(f"Civitai APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                else:
                    return None
        
        return None
    
    async def _search_civarchive_api(self, sha256: str) -> Optional[Dict]:
        """CivArchive APIã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            # CivArchiveã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            url = f"https://civarchive.com/api/sha256/{sha256.lower()}"
            async with self.session.get(url, headers=self._get_headers(use_auth=False)) as response:
                if response.status == 200:
                    data = await response.json()
                    logger.info(f"CivArchiveã§æ¤œç´¢æˆåŠŸ: {data.get('name', 'Unknown')}")
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                    logger.debug(f"CivArchiveãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ : {list(data.keys())}")
                    if 'files' in data:
                        logger.debug(f"filesé…åˆ—: {len(data['files'])}å€‹")
                        for i, file_info in enumerate(data['files']):
                            logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ« {i}: {file_info}")
                    
                    return data
                elif response.status == 404:
                    logger.debug(f"CivArchive: ãƒãƒƒã‚·ãƒ¥ {sha256[:16]}... ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return None
                else:
                    logger.warning(f"CivArchive: ã‚¨ãƒ©ãƒ¼ {response.status}")
                    return None
        except Exception as e:
            logger.error(f"CivArchiveæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _get_model_version_info(self, model_id: int, version_id: Optional[int] = None) -> Optional[Dict]:
        """ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—"""
        if not self.session:
            return None
        
        try:
            if version_id:
                url = f"{self.base_url}/model-versions/{version_id}"
            else:
                url = f"{self.base_url}/models/{model_id}"
            
            async with self.session.get(url, headers=self._get_headers()) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    logger.warning(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {response.status}")
                    return None
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_download_urls(self, model_info: Dict) -> List[str]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’æŠ½å‡º"""
        download_urls = []
        
        try:
            logger.debug(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLæŠ½å‡ºé–‹å§‹: {list(model_info.keys())}")
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
            if 'files' in model_info:
                logger.debug(f"filesé…åˆ—: {len(model_info['files'])}å€‹")
                for i, file_info in enumerate(model_info['files']):
                    logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ« {i}: {file_info.get('name', 'Unknown')} (type: {file_info.get('type')}, primary: {file_info.get('primary')})")
                    
                    if file_info.get('type') == 'Model' and file_info.get('primary'):
                        # ãƒ¡ã‚¤ãƒ³ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL
                        if 'downloadUrl' in file_info and file_info['downloadUrl']:
                            download_urls.append(file_info['downloadUrl'])
                            logger.debug(f"ãƒ¡ã‚¤ãƒ³URLè¿½åŠ : {file_info['downloadUrl']}")
                        
                        # ãƒŸãƒ©ãƒ¼URL
                        if 'mirrors' in file_info:
                            logger.debug(f"ãƒŸãƒ©ãƒ¼æ•°: {len(file_info['mirrors'])}")
                            for j, mirror in enumerate(file_info['mirrors']):
                                if mirror.get('url') and not mirror.get('deletedAt'):
                                    download_urls.append(mirror['url'])
                                    logger.debug(f"ãƒŸãƒ©ãƒ¼URLè¿½åŠ : {mirror['url']}")
            
            # CivArchiveã®å ´åˆã¯ç•°ãªã‚‹æ§‹é€ ã®å¯èƒ½æ€§
            if not download_urls and 'downloadUrl' in model_info:
                download_urls.append(model_info['downloadUrl'])
                logger.debug(f"CivArchiveãƒ¡ã‚¤ãƒ³URLè¿½åŠ : {model_info['downloadUrl']}")
            
            # é‡è¤‡ã‚’é™¤å»
            download_urls = list(set(download_urls))
            
            logger.info(f"æŠ½å‡ºã•ã‚ŒãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL: {len(download_urls)}å€‹")
            for i, url in enumerate(download_urls):
                logger.info(f"  {i+1}. {url}")
            
        except Exception as e:
            logger.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        
        return download_urls
    
    async def scan_model_file(self, file_path: str) -> Optional[ModelMetadata]:
        """
        å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        
        Args:
            file_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            ModelMetadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€å–å¾—å¤±æ•—æ™‚ã¯None
        """
        if not os.path.exists(file_path):
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
            return None
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path)
        
        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        _, ext = os.path.splitext(file_name)
        if ext.lower() not in self.supported_extensions:
            logger.warning(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„æ‹¡å¼µå­: {ext}")
            return None
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’åˆ¤å®š
        model_type = self._detect_model_type(file_path, file_name)
        base_model = self._detect_base_model(file_path, file_name)
        
        # SHA256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        sha256 = self._calculate_sha256(file_path)
        
        # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        metadata = ModelMetadata(
            file_name=file_name,
            file_path=file_path,
            file_size=file_size,
            sha256=sha256,
            model_type=model_type,
            base_model=base_model
        )
        
        # Civitaiã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        if sha256 and self.session:
            try:
                logger.info(f"Civitaiæ¤œç´¢é–‹å§‹: SHA256={sha256[:16]}...")
                
                # ãƒãƒƒã‚·ãƒ¥ã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
                model_info = await self._search_model_by_hash(sha256)
                
                if model_info:
                    logger.info(f"Civitaiæ¤œç´¢æˆåŠŸ: {model_info.get('name', 'Unknown')}")
                    metadata.from_civitai = True
                    metadata.model_id = model_info.get('modelId')
                    metadata.model_name = model_info.get('name')
                    metadata.description = model_info.get('model', {}).get('description')
                    metadata.tags = model_info.get('model', {}).get('tags', [])
                    metadata.creator = model_info.get('creator', {}).get('username') if model_info.get('creator') else None
                    metadata.nsfw_level = model_info.get('nsfw', 0)
                    metadata.version_id = model_info.get('id')
                    
                    if metadata.model_id and metadata.version_id:
                        metadata.civitai_url = f"https://civitai.com/models/{metadata.model_id}?modelVersionId={metadata.version_id}"
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’æŠ½å‡º
                    metadata.download_urls = self._extract_download_urls(model_info)
                    logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLæŠ½å‡º: {len(metadata.download_urls)}å€‹")
                else:
                    logger.info(f"Civitaiæ¤œç´¢å¤±æ•—: SHA256={sha256[:16]}...")
                
            except Exception as e:
                logger.error(f"Civitaiãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            if not sha256:
                logger.warning(f"SHA256ãƒãƒƒã‚·ãƒ¥ãŒç©ºã§ã™: {file_path}")
            if not self.session:
                logger.warning(f"HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return metadata
    
    async def scan_directory(self, directory_path: str, recursive: bool = True) -> List[ModelMetadata]:
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        
        Args:
            directory_path: ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            recursive: å†å¸°çš„ã«ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ã‹
            
        Returns:
            List[ModelMetadata]: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        if not os.path.exists(directory_path):
            logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {directory_path}")
            return []
        
        metadata_list = []
        
        try:
            if recursive:
                for root, dirs, files in os.walk(directory_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        _, ext = os.path.splitext(file)
                        if ext.lower() in self.supported_extensions:
                            metadata = await self.scan_model_file(file_path)
                            if metadata:
                                metadata_list.append(metadata)
            else:
                for file in os.listdir(directory_path):
                    file_path = os.path.join(directory_path, file)
                    if os.path.isfile(file_path):
                        _, ext = os.path.splitext(file)
                        if ext.lower() in self.supported_extensions:
                            metadata = await self.scan_model_file(file_path)
                            if metadata:
                                metadata_list.append(metadata)
        
        except Exception as e:
            logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¹ã‚­ãƒ£ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        
        return metadata_list
    
    def classify_by_type(self, metadata_list: List[ModelMetadata]) -> Dict[str, List[ModelMetadata]]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã«åˆ†é¡"""
        classified = {
            'lora': [],
            'checkpoint': [],
            'embedding': [],
            'unknown': []
        }
        
        for metadata in metadata_list:
            model_type = metadata.model_type.lower()
            if model_type in classified:
                classified[model_type].append(metadata)
            else:
                classified['unknown'].append(metadata)
        
        return classified
    
    def extract_download_urls(self, metadata_list: List[ModelMetadata]) -> Dict[str, List[str]]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’æŠ½å‡ºãƒ»åˆ†é¡"""
        urls_by_type = {
            'lora': [],
            'checkpoint': [],
            'embedding': [],
            'unknown': []
        }
        
        for metadata in metadata_list:
            model_type = metadata.model_type.lower()
            if model_type in urls_by_type:
                urls_by_type[model_type].extend(metadata.download_urls)
            else:
                urls_by_type['unknown'].extend(metadata.download_urls)
        
        # é‡è¤‡ã‚’é™¤å»
        for model_type in urls_by_type:
            urls_by_type[model_type] = list(set(urls_by_type[model_type]))
        
        return urls_by_type
    
    def save_metadata_to_json(self, metadata_list: List[ModelMetadata], output_path: str):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            data = []
            for metadata in metadata_list:
                data.append(asdict(metadata))
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        
        except Exception as e:
            logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def load_metadata_from_json(self, input_path: str) -> List[ModelMetadata]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            metadata_list = []
            for item in data:
                metadata = ModelMetadata(**item)
                metadata_list.append(metadata)
            
            logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {input_path}")
            return metadata_list
        
        except Exception as e:
            logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def save_to_download_history_csv(self, metadata_list: List[ModelMetadata], output_path: str):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’download_history.csvå½¢å¼ã§ä¿å­˜"""
        try:
            import csv
            from datetime import datetime
            
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
                writer.writerow([
                    'timestamp', 'model_type', 'url', 'filename', 'model_id', 
                    'version_id', 'file_size', 'file_size_bytes'
                ])
                
                # ãƒ‡ãƒ¼ã‚¿è¡Œ
                for metadata in metadata_list:
                    if metadata.download_urls:
                        for download_url in metadata.download_urls:
                            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’GBã¨ãƒã‚¤ãƒˆã§è¡¨ç¤º
                            file_size_gb = metadata.file_size / (1024**3)
                            file_size_str = f"{file_size_gb:.2f} GB"
                            
                            # Civitai URLã‚’å–å¾—ï¼ˆmodel_idã¨version_idãŒã‚ã‚‹å ´åˆã¯æ­£ã—ã„URLå½¢å¼ã§ç”Ÿæˆï¼‰
                            if metadata.civitai_url:
                                civitai_url = metadata.civitai_url
                            elif metadata.model_id and metadata.version_id:
                                civitai_url = f"https://civitai.com/models/{metadata.model_id}?modelVersionId={metadata.version_id}"
                            else:
                                civitai_url = download_url
                            
                            writer.writerow([
                                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                metadata.model_type,
                                civitai_url,
                                metadata.file_name,
                                metadata.model_id or '',
                                metadata.version_id or '',
                                file_size_str,
                                str(metadata.file_size)
                            ])
            
            logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’CSVå½¢å¼ã§ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        
        except Exception as e:
            logger.error(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def extract_download_urls_for_csv(self, metadata_list: List[ModelMetadata]) -> List[Dict]:
        """CSVå‡ºåŠ›ç”¨ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLæƒ…å ±ã‚’æŠ½å‡ºï¼ˆdownload_history.csvå½¢å¼ï¼‰"""
        download_entries = []
        
        for metadata in metadata_list:
            if metadata.download_urls:
                for download_url in metadata.download_urls:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’GBã¨ãƒã‚¤ãƒˆã§è¡¨ç¤º
                    file_size_gb = metadata.file_size / (1024**3)
                    file_size_str = f"{file_size_gb:.2f} GB"
                    
                    # Civitai URLã‚’å–å¾—ï¼ˆmodel_idã¨version_idãŒã‚ã‚‹å ´åˆã¯æ­£ã—ã„URLå½¢å¼ã§ç”Ÿæˆï¼‰
                    if metadata.civitai_url:
                        civitai_url = metadata.civitai_url
                    elif metadata.model_id and metadata.version_id:
                        civitai_url = f"https://civitai.com/models/{metadata.model_id}?modelVersionId={metadata.version_id}"
                    else:
                        civitai_url = download_url
                    
                    entry = {
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'model_type': metadata.model_type,
                        'url': civitai_url,
                        'filename': metadata.file_name,
                        'model_id': metadata.model_id or '',
                        'version_id': metadata.version_id or '',
                        'file_size': file_size_str,
                        'file_size_bytes': str(metadata.file_size)
                    }
                    download_entries.append(entry)
        
        return download_entries
    
    def extract_detailed_metadata_for_csv(self, metadata_list: List[ModelMetadata]) -> List[Dict]:
        """è©³ç´°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVå‡ºåŠ›ç”¨ã«æŠ½å‡ºï¼ˆæ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»˜ãï¼‰"""
        detailed_entries = []
        
        for metadata in metadata_list:
            if metadata.download_urls:
                for download_url in metadata.download_urls:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’GBã¨ãƒã‚¤ãƒˆã§è¡¨ç¤º
                    file_size_gb = metadata.file_size / (1024**3)
                    file_size_str = f"{file_size_gb:.2f} GB"
                    
                    # Civitai URLã‚’å–å¾—ï¼ˆmodel_idãŒã‚ã‚‹å ´åˆï¼‰
                    civitai_url = metadata.civitai_url or download_url
                    
                    entry = {
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'model_type': metadata.model_type,
                        'url': civitai_url,
                        'filename': metadata.file_name,
                        'model_id': metadata.model_id or '',
                        'version_id': metadata.version_id or '',
                        'file_size': file_size_str,
                        'file_size_bytes': str(metadata.file_size),
                        'model_name': metadata.model_name or '',
                        'creator': metadata.creator or '',
                        'base_model': metadata.base_model or '',
                        'sha256': metadata.sha256,
                        'download_url': download_url,
                        'nsfw_level': metadata.nsfw_level,
                        'tags': ', '.join(metadata.tags) if metadata.tags else '',
                        'description': metadata.description or ''
                    }
                    detailed_entries.append(entry)
        
        return detailed_entries


# ä½¿ç”¨ä¾‹
async def main():
    """ä½¿ç”¨ä¾‹"""
    import json
    
    # config.jsonã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
    config_path = "config.json"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        api_key = config.get('civitai_api_key', 'YOUR_API_KEY_HERE')
        download_paths = config.get('download_paths', {})
        
        print(f"ğŸ”‘ APIã‚­ãƒ¼: {'è¨­å®šæ¸ˆã¿' if api_key != 'YOUR_API_KEY_HERE' else 'æœªè¨­å®š'}")
        print(f"ğŸ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¹: {download_paths}")
        
    except FileNotFoundError:
        print(f"âŒ config.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        return
    except Exception as e:
        print(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¹ã‚­ãƒ£ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
    async with ModelMetadataScanner(api_key=api_key) as scanner:
        # å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
        all_metadata = []
        
        for model_type, directory in download_paths.items():
            print(f"\nğŸ” {model_type.upper()}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³: {directory}")
            if os.path.exists(directory):
                metadata_list = await scanner.scan_directory(directory, recursive=True)
                all_metadata.extend(metadata_list)
                print(f"âœ… {len(metadata_list)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")
            else:
                print(f"âš ï¸  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {directory}")
        
        if all_metadata:
            # ã‚¿ã‚¤ãƒ—åˆ¥ã«åˆ†é¡
            classified = scanner.classify_by_type(all_metadata)
            print(f"\nğŸ“Š åˆ†é¡çµæœ:")
            for model_type, items in classified.items():
                if items:
                    print(f"  {model_type.capitalize()}: {len(items)}å€‹")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’æŠ½å‡º
            urls_by_type = scanner.extract_download_urls(all_metadata)
            print(f"\nğŸ”— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL:")
            for model_type, urls in urls_by_type.items():
                if urls:
                    print(f"  {model_type.capitalize()}: {len(urls)}å€‹")
            
            # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            output_file = "model_metadata_results.json"
            scanner.save_metadata_to_json(all_metadata, output_file)
            print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
        else:
            print(f"\nâŒ ã‚¹ã‚­ãƒ£ãƒ³ã§ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    asyncio.run(main())
