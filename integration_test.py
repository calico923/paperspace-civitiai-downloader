#!/usr/bin/env python3
"""
çµ±åˆãƒ†ã‚¹ãƒˆ: å®Ÿéš›ã®download_history.csvã«æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ 

æ—¢å­˜ã®download_history.csvã«æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ã—ã¦
å®Œå…¨ãªçµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹
"""

import asyncio
import json
import os
import sys
import csv
from datetime import datetime
from pathlib import Path

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from model_metadata_scanner import ModelMetadataScanner

async def integration_test():
    """çµ±åˆãƒ†ã‚¹ãƒˆ: å®Ÿéš›ã®download_history.csvã«æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ """
    
    print("ğŸš€ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ï¼")
    print("ğŸ“‹ æ—¢å­˜ã®download_history.csvã«æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ã—ã¾ã™")
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
    test_file = "/Users/kuniaki-k/Code/paperspace/civitai_downloader/downloads/checkpoints/waiIllustriousSDXL_v150.safetensors"
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
    config_path = "config.json"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        api_key = config.get('civitai_api_key', 'YOUR_API_KEY_HERE')
        
        if api_key == 'YOUR_API_KEY_HERE':
            print("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
            
    except Exception as e:
        print(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # æ—¢å­˜ã®download_history.csvã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    history_file = "download_history.csv"
    backup_file = "download_history.csv.backup"
    
    if os.path.exists(history_file):
        import shutil
        shutil.copy2(history_file, backup_file)
        print(f"ğŸ“ æ—¢å­˜ã®å±¥æ­´ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
    
    # ã‚¹ã‚­ãƒ£ãƒŠãƒ¼ã‚’åˆæœŸåŒ–ã—ã¦ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    async with ModelMetadataScanner(api_key=api_key) as scanner:
        print(f"\nğŸ” ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹...")
        
        try:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            metadata = await scanner.scan_model_file(test_file)
            
            if metadata and metadata.download_urls:
                print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºæˆåŠŸï¼")
                print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å: {metadata.file_name}")
                print(f"ğŸ†” ãƒ¢ãƒ‡ãƒ«ID: {metadata.model_id}")
                print(f"ğŸ”¢ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ID: {metadata.version_id}")
                print(f"ğŸ”— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL: {len(metadata.download_urls)}å€‹")
                
                # æ—¢å­˜ã®download_history.csvã«æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ 
                print(f"\nğŸ“ æ—¢å­˜ã®download_history.csvã«æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ...")
                
                # æ—¢å­˜ã®CSVã‚’èª­ã¿è¾¼ã¿
                existing_entries = []
                if os.path.exists(history_file):
                    with open(history_file, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        existing_entries = list(reader)
                    print(f"ğŸ“Š æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(existing_entries)}")
                
                # æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
                new_entries = scanner.extract_download_urls_for_csv([metadata])
                print(f"ğŸ†• æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(new_entries)}")
                
                # æ—¢å­˜ã®ã‚¨ãƒ³ãƒˆãƒªã¨æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’çµåˆ
                all_entries = existing_entries + new_entries
                print(f"ğŸ“ˆ åˆè¨ˆã‚¨ãƒ³ãƒˆãƒªæ•°: {len(all_entries)}")
                
                # æ›´æ–°ã•ã‚ŒãŸCSVã‚’ä¿å­˜
                with open(history_file, 'w', newline='', encoding='utf-8') as f:
                    if all_entries:
                        fieldnames = all_entries[0].keys()
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        writer.writerows(all_entries)
                
                print(f"âœ… download_history.csvã‚’æ›´æ–°ã—ã¾ã—ãŸ: {history_file}")
                
                # æ›´æ–°ã•ã‚ŒãŸCSVã®å†…å®¹ã‚’è¡¨ç¤º
                print(f"\nğŸ“‹ æ›´æ–°ã•ã‚ŒãŸdownload_history.csvã®å†…å®¹:")
                with open(history_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    print(content)
                
                # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
                print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
                print(f"  ğŸ“ ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(all_entries)}")
                
                # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆ
                type_counts = {}
                for entry in all_entries:
                    model_type = entry.get('model_type', 'unknown')
                    type_counts[model_type] = type_counts.get(model_type, 0) + 1
                
                for model_type, count in type_counts.items():
                    print(f"  ğŸ·ï¸  {model_type}: {count}å€‹")
                
                print(f"\nğŸ‰ çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
                print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«: {backup_file}")
                print(f"ğŸ“„ æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {history_file}")
                
            else:
                print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            print(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    await integration_test()

if __name__ == "__main__":
    asyncio.run(main())
